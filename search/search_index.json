{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to RQEquilibrium Documentation","text":"<p>This is the homepage for the RQEquilibrium library documentation.</p> <p>This is an implementation of the Risk-Averse Quantal Equilibrium (RQE) solution concepts. Which was introduced in the paper: Tractable Multi-Agent Reinforcement Learning Through Behavioral Economics and published as conference paper at ICLR 2025.</p> <p>If you want to cite the orinal paper, please use the following BibTeX entry:</p> <pre><code>@inproceedings{\nmazumdar2025tractable,\ntitle={Tractable Multi-Agent Reinforcement Learning through Behavioral Economics},\nauthor={Eric Mazumdar, Kishan Panaganti, Laixi Shi},\nbooktitle={The Thirteenth International Conference on Learning Representations},\nyear={2025},\nurl={https://openreview.net/forum?id=stUKwWBuBm}\n}\n</code></pre>"},{"location":"index.html#usage","title":"Usage","text":"<p>Here's a basic example of how to use the library:</p> <pre><code>from rqequilibrium import RQE, Player\n\nR1 = np.array([[200, 160], [370, 10]])\nR2 = np.array([[160, 10], [200, 370]])\nR3 = R1.copy()\nR4 = R2.copy()\nRQE.print_game(R1, R2)\nplayers = [\n    Player(tau=0.001, epsilon=170, game_matrix=R1),\n    Player(tau=0.06, epsilon=110, game_matrix=R2),\n    Player(tau=0.003, epsilon=190, game_matrix=R3),\n    Player(tau=0.05, epsilon=130, game_matrix=R4),\n]\nrqe_solver = RQE(players=players, lr=1e-4, max_iter=1000, br_iters=50)\n\nprint(\"Computed policies for RQE\")\n# print(\"Player 1:\", np.argmax(pi1), \"with policy:\", pi1)\ni = 0\nfor pi in rqe_solver.optimize():\n    print(f\"Player {i + 1}: Best Action {np.argmax(pi)} with policy: {pi}\")\n    i += 1\n\n</code></pre>"},{"location":"index.html#todos","title":"TODOs","text":"<ul> <li> Implementing a simple RQE solver.<ul> <li> Implementing the D functions and v functions implemented in the paper.</li> <li> Implementing the Projected Gradient Descent.</li> </ul> </li> <li> Testing the RQE solver.<ul> <li> Testing using the original Paper Clif Walking.</li> <li> Testing using the original Paper 13 games matrix.</li> <li> Testing using Google's Deepmind OpenSpiel library.</li> <li> Testing the n player games.</li> </ul> </li> <li> Adding more features.<ul> <li> Working for n player games.</li> <li> Make it scikit-learn compatible.</li> <li> Using JAX for faster computation on bigger games.</li> </ul> </li> </ul>"},{"location":"reference/opt.html","title":"Optimization","text":""},{"location":"reference/opt.html#rqequilibrium.opt","title":"rqequilibrium.opt","text":""},{"location":"reference/opt.html#rqequilibrium.opt.ProjectedGradientDescent","title":"ProjectedGradientDescent","text":"<p>A class for performing projected gradient descent to solve optimization problems.</p> <p>Attributes:</p> Name Type Description <code>lr</code> <code>float</code> <p>Learning rate for the gradient descent step.</p> <code>projection</code> <code>Callable</code> <p>Function to project the point onto a feasible set after each step.</p> <p>Methods:</p> Name Description <code>step</code> <p>np.ndarray, gradients_values: np.ndarray) -&gt; np.ndarray: Performs a single step of projected gradient descent. Updates the current point <code>w</code> by subtracting the scaled gradients and projecting it onto the feasible set.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>class ProjectedGradientDescent:\n    \"\"\"\n    A class for performing projected gradient descent to solve optimization problems.\n\n    Attributes:\n        lr (float): Learning rate for the gradient descent step.\n        projection (Callable): Function to project the point onto a feasible set after each step.\n\n    Methods:\n        step(w: np.ndarray, gradients_values: np.ndarray) -&gt; np.ndarray:\n            Performs a single step of projected gradient descent.\n            Updates the current point `w` by subtracting the scaled gradients and projecting it onto the feasible set.\n    \"\"\"\n\n    def __init__(\n        self,\n        lr: float = 0.1,\n        projection=lambda x: x,\n    ):\n\n        self.lr = lr\n        self.projection = projection\n\n    def step(self, w: np.ndarray, gradients_values: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Perform a single step of projected gradient descent.\n\n        Parameters:\n            w (np.ndarray): The current point.\n            gradients_values (np.ndarray): The computed gradients at the current point.\n        Returns:\n            The updated point after one step.\n        \"\"\"\n\n        w -= self.lr * gradients_values\n        return self.projection(np.clip(w, 1e-12, 1 - 1e-12))\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.ProjectedGradientDescent.step","title":"step","text":"<pre><code>step(w: ndarray, gradients_values: ndarray) -&gt; np.ndarray\n</code></pre> <p>Perform a single step of projected gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>ndarray</code> <p>The current point.</p> required <code>gradients_values</code> <code>ndarray</code> <p>The computed gradients at the current point.</p> required <p>Returns:     The updated point after one step.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def step(self, w: np.ndarray, gradients_values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Perform a single step of projected gradient descent.\n\n    Parameters:\n        w (np.ndarray): The current point.\n        gradients_values (np.ndarray): The computed gradients at the current point.\n    Returns:\n        The updated point after one step.\n    \"\"\"\n\n    w -= self.lr * gradients_values\n    return self.projection(np.clip(w, 1e-12, 1 - 1e-12))\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(p: ndarray, q: ndarray) -&gt; float\n</code></pre> <p>Compute the KL divergence between two probability distributions p and q.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The first probability distribution.</p> required <code>q</code> <code>ndarray</code> <p>The second probability distribution.</p> required <p>Returns:     The KL divergence between p and q.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def kl_divergence(p: np.ndarray, q: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the KL divergence between two probability distributions p and q.\n\n    Args:\n        p (np.ndarray): The first probability distribution.\n        q (np.ndarray): The second probability distribution.\n    Returns:\n        The KL divergence between p and q.\n    \"\"\"\n    if q.ndim &gt; 1:\n        return np.sum([kl_divergence(p, q_i) for q_i in q])\n\n    return np.sum(p * (np.log(p) - np.log(q)))\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.kl_reversed","title":"kl_reversed","text":"<pre><code>kl_reversed(p: ndarray, q: ndarray) -&gt; float\n</code></pre> <p>Compute the reversed KL divergence between two probability distributions p and q.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The first probability distribution.</p> required <code>q</code> <code>ndarray</code> <p>The second probability distribution.</p> required <p>Returns:     The reversed KL divergence between p and q.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def kl_reversed(p: np.ndarray, q: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the reversed KL divergence between two probability distributions p and q.\n\n    Args:\n        p (np.ndarray): The first probability distribution.\n        q (np.ndarray): The second probability distribution.\n    Returns:\n        The reversed KL divergence between p and q.\n    \"\"\"\n\n    return kl_divergence(q, p)\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.log_barrier","title":"log_barrier","text":"<pre><code>log_barrier(p: ndarray) -&gt; float\n</code></pre> <p>Compute the log barrier function for a probability distribution p. Args:     p (np.ndarray): The probability distribution. Returns:     The log barrier value for the distribution.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def log_barrier(p: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the log barrier function for a probability distribution p.\n    Args:\n        p (np.ndarray): The probability distribution.\n    Returns:\n        The log barrier value for the distribution.\n    \"\"\"\n    return -np.sum(np.log(p))\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.negative_entropy","title":"negative_entropy","text":"<pre><code>negative_entropy(p: ndarray) -&gt; float\n</code></pre> <p>Compute the negative entropy of a probability distribution p. Args:     p (np.ndarray): The probability distribution. Returns:     The negative entropy of the distribution.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def negative_entropy(p: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the negative entropy of a probability distribution p.\n    Args:\n        p (np.ndarray): The probability distribution.\n    Returns:\n        The negative entropy of the distribution.\n    \"\"\"\n    return -np.sum(p * np.log(p))\n</code></pre>"},{"location":"reference/opt.html#rqequilibrium.opt.project_simplex","title":"project_simplex","text":"<pre><code>project_simplex(v: ndarray, s=1) -&gt; np.ndarray\n</code></pre> <p>Projects a vector v onto the simplex defined by the sum of its components being equal to s. The simplex is defined as the set of vectors x such that: x &gt;= 0 and sum(x) = s.</p> <p>Taken from https://gist.github.com/daien/1272551.</p> <p>Basically Implements Corollary 6.29 First Order Optimization, Amir Beck</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>ndarray</code> <p>The vector to be projected.</p> required <code>s</code> <code>float</code> <p>The sum that the components of the projected vector should equal. Default is 1.</p> <code>1</code> <p>Returns:     The projected vector onto the simplex.</p> Source code in <code>src/rqequilibrium/opt.py</code> <pre><code>def project_simplex(v: np.ndarray, s=1) -&gt; np.ndarray:\n    \"\"\"\n    Projects a vector v onto the simplex defined by the sum of its components being equal to s.\n    The simplex is defined as the set of vectors x such that:\n    x &gt;= 0 and sum(x) = s.\n\n    Taken from [https://gist.github.com/daien/1272551](https://gist.github.com/daien/1272551).\n\n    Basically Implements Corollary 6.29 First Order Optimization, Amir Beck\n\n    Args:\n        v (np.ndarray): The vector to be projected.\n        s (float): The sum that the components of the projected vector should equal. Default is 1.\n    Returns:\n        The projected vector onto the simplex.\n    \"\"\"\n    assert s &gt; 0, \"Radius s must be strictly positive (%d &lt;= 0)\" % s\n    v = np.reshape(v, (v.shape[0]))\n    (n,) = v.shape  # will raise ValueError if v is not 1-D\n    # check if we are already on the simplex\n    if v.sum() == s and np.all(v &gt;= 0):\n        # best projection: itself!\n        return v\n    # get the array of cumulative sums of a sorted (decreasing) copy of v\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # get the number of &gt; 0 components of the optimal solution\n    rho = np.nonzero(u * np.arange(1, n + 1) &gt; (cssv - s))[0][-1]\n    # compute the Lagrange multiplier associated to the simplex constraint\n    theta = float(cssv[rho] - s) / (rho + 1)\n    # compute the projection by thresholding v using theta\n    w = (v - theta).clip(min=0)\n    return w\n</code></pre>"},{"location":"reference/rqe.html","title":"Risk-Averse Quantal Equilibrium (RQE)","text":""},{"location":"reference/rqe.html#rqequilibrium.rqe","title":"rqequilibrium.rqe","text":""},{"location":"reference/rqe.html#rqequilibrium.rqe.Player","title":"Player  <code>dataclass</code>","text":"<p>Concept of a player in the RQE game.</p> <p>Attributes:</p> Name Type Description <code>tau</code> <code>float</code> <p>Risk aversion parameter.</p> <code>epsilon</code> <code>float</code> <p>Bounded Rational parameter.</p> <code>game_matrix</code> <code>ndarray</code> <p>The payoff matrix for the player.</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>@dataclass\nclass Player:\n    \"\"\"\n    Concept of a player in the RQE game.\n\n    Attributes:\n        tau: Risk aversion parameter.\n        epsilon: Bounded Rational parameter.\n        game_matrix: The payoff matrix for the player.\n    \"\"\"\n\n    tau: float\n    epsilon: float\n    game_matrix: np.ndarray\n</code></pre>"},{"location":"reference/rqe.html#rqequilibrium.rqe.RQE","title":"RQE","text":"<p>RQE (Risk Quantal Response Equilibrium) solver for multi-player games.</p> <p>This class implements the RQE solution concept, which combines risk aversion and bounded rationality in a multi-player setting. It uses projected gradient descent to optimize the policies of players.</p> <p>Attributes:</p> Name Type Description <code>players</code> <code>list[Player]</code> <p>List of Player objects representing the players in the game.</p> <code>lr</code> <code>float</code> <p>Learning rate for the optimization.</p> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations for the optimization.</p> <code>quantal_function</code> <code>Callable</code> <p>Function to compute the quantal response.</p> <code>risk_function</code> <code>Callable</code> <p>Function to compute the risk term.</p> <code>projection</code> <code>Callable</code> <p>Function to project policies onto a simplex.</p> <p>Methods:</p> Name Description <code>risk_term</code> <p>Computes the risk term for a player given the game matrix, policy, and other player's policy.</p> <code>quantal_term</code> <p>Computes the quantal response term for a player given the game matrix, policy, and epsilon parameter.</p> <code>optimize</code> <p>Optimizes the policies for all players using projected gradient descent.</p> <code>print_game</code> <p>Prints the game matrices for two player games.</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>class RQE:\n    \"\"\"\n    RQE (Risk Quantal Response Equilibrium) solver for multi-player games.\n\n    This class implements the RQE solution concept, which combines risk aversion and bounded rationality\n    in a multi-player setting. It uses projected gradient descent to optimize the policies of players.\n\n    Attributes:\n        players (list[Player]): List of Player objects representing the players in the game.\n        lr (float): Learning rate for the optimization.\n        max_iter (int): Maximum number of iterations for the optimization.\n        quantal_function: Function to compute the quantal response.\n        risk_function: Function to compute the risk term.\n        projection: Function to project policies onto a simplex.\n\n    Methods:\n        risk_term: Computes the risk term for a player given the game matrix, policy, and other player's policy.\n        quantal_term: Computes the quantal response term for a player given the game matrix, policy, and epsilon parameter.\n        optimize: Optimizes the policies for all players using projected gradient descent.\n        print_game: Prints the game matrices for two player games.\n    \"\"\"\n\n    quantal_function: Callable\n    risk_function: Callable\n    players: List[Player]\n    projection: Callable\n    lr: float\n    max_iter: int\n    br_iters: int\n    EPS: float = 1e-12\n\n    def __init__(\n        self,\n        players: List[Player],\n        lr: float = 0.1,\n        max_iter: int = 500,\n        br_iters: int = 50,\n        quantal_function: Union[Callable, str] = \"log_barrier\",\n        risk_function: Union[Callable, str] = \"kl_divergence\",\n        projection: Callable = project_simplex,\n    ):\n        self.players = players\n        self.lr = lr\n        self.max_iter = max_iter\n        self.br_iters = br_iters\n        self.EPS = 1e-12\n        self.projection = projection\n\n        if hasattr(quantal_function, \"__call__\"):\n            self.quantal_function = quantal_function\n        elif quantal_function == \"negative_entropy\":\n            self.quantal_function = negative_entropy\n        elif quantal_function == \"log_barrier\":\n            self.quantal_function = log_barrier\n        else:\n            raise ValueError(\"Invalid quantal function specified.\")\n\n        if hasattr(risk_function, \"__call__\"):\n            self.risk_function = risk_function\n        elif risk_function == \"kl_divergence\":\n            self.risk_function = kl_divergence\n        elif risk_function == \"kl_reversed\":\n            self.risk_function = kl_reversed\n\n        self.grad_risk = grad(self.risk_function)\n        self.grad_quantal = grad(self.quantal_function)\n\n    def risk_term(\n        self, game: np.ndarray, x: np.ndarray, p: np.ndarray, y: np.ndarray, tau: float\n    ) -&gt; np.array:\n        \"\"\"\n        Compute the risk term for a player given the game matrix, policy, and other player's policy.\n\n        Parameters:\n            game: The game matrix for the player.\n            x: The current policy of the player.\n            p: The last risk aversion term.\n            y: The policy of all the other players.\n        Returns:\n            The gradient of the risk term\n        \"\"\"\n        return game.T @ x + (1 / tau) * self.grad_risk(p, y)\n\n    def quantal_term(\n        self, game: np.ndarray, x: np.ndarray, p: np.ndarray, epsilon: float\n    ) -&gt; np.array:\n        \"\"\"\n        Compute the quantal response term for a player given the game matrix, policy and epsilon parameter.\n        Parameters:\n            game: The game matrix for the player.\n            x: The current policy of the player.\n            p: The risk aversion term.\n            epsilon: The epsilon parameter for bounded rationality.\n        Returns:\n            The gradient of the quantal term\n        \"\"\"\n        return -game @ p + epsilon * self.grad_quantal(x)\n\n    def optimize(self) -&gt; np.ndarray:\n        \"\"\"\n        Optimize the policies for both players using projected gradient descent.\n\n        Returns:\n            The optimal policy using RQE\n        \"\"\"\n\n        num_players = len(self.players)  # Number of players\n        max_action_set = max(player.game_matrix.shape[1] for player in self.players)\n\n        # Initialize the Projected Gradient Descent optimizer\n        pgd = ProjectedGradientDescent(\n            lr=self.lr,\n            projection=self.projection,\n        )\n\n        # Initialize random policies for both players\n        policies = np.random.rand(num_players, max_action_set)\n        risk_policies = np.random.rand(num_players, max_action_set)\n        policies /= np.sum(policies, axis=1, keepdims=True)\n        risk_policies /= np.sum(risk_policies, axis=1, keepdims=True)\n\n        for _ in range(self.max_iter):\n            # Compute the quantal and risk terms for both players\n            policies_buff = policies.copy()\n            risk_buff = risk_policies.copy()\n            for i, player in enumerate(self.players):\n                game = player.game_matrix if i % 2 == 0 else player.game_matrix\n\n                quantal_grad = self.quantal_term(\n                    game, policies_buff[i], risk_buff[i], player.epsilon\n                )\n                opponnet_policies = np.delete(policies_buff, i, axis=0)\n\n                risk_grad = self.risk_term(\n                    game,\n                    policies_buff[i],\n                    risk_buff[i],\n                    opponnet_policies[0],\n                    player.tau,\n                )\n\n                policies_buff[i] = pgd.step(policies[i], quantal_grad)\n                risk_buff[i] = pgd.step(risk_policies[i], risk_grad)\n\n            risk_policies = risk_buff\n            policies = policies_buff\n\n        return policies\n\n    @staticmethod\n    def print_game(R1: np.ndarray, R2: np.ndarray):\n        \"\"\"\n        Print the game matrices for both players.\n        \"\"\"\n        for i in range(R1.shape[0]):\n            row = []\n            for j in range(R1.shape[1]):\n                row.append(f\"{int(R1[i, j])}, {int(R2[i, j])}\")\n            print(\" | \".join(row))\n</code></pre>"},{"location":"reference/rqe.html#rqequilibrium.rqe.RQE.optimize","title":"optimize","text":"<pre><code>optimize() -&gt; np.ndarray\n</code></pre> <p>Optimize the policies for both players using projected gradient descent.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The optimal policy using RQE</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>def optimize(self) -&gt; np.ndarray:\n    \"\"\"\n    Optimize the policies for both players using projected gradient descent.\n\n    Returns:\n        The optimal policy using RQE\n    \"\"\"\n\n    num_players = len(self.players)  # Number of players\n    max_action_set = max(player.game_matrix.shape[1] for player in self.players)\n\n    # Initialize the Projected Gradient Descent optimizer\n    pgd = ProjectedGradientDescent(\n        lr=self.lr,\n        projection=self.projection,\n    )\n\n    # Initialize random policies for both players\n    policies = np.random.rand(num_players, max_action_set)\n    risk_policies = np.random.rand(num_players, max_action_set)\n    policies /= np.sum(policies, axis=1, keepdims=True)\n    risk_policies /= np.sum(risk_policies, axis=1, keepdims=True)\n\n    for _ in range(self.max_iter):\n        # Compute the quantal and risk terms for both players\n        policies_buff = policies.copy()\n        risk_buff = risk_policies.copy()\n        for i, player in enumerate(self.players):\n            game = player.game_matrix if i % 2 == 0 else player.game_matrix\n\n            quantal_grad = self.quantal_term(\n                game, policies_buff[i], risk_buff[i], player.epsilon\n            )\n            opponnet_policies = np.delete(policies_buff, i, axis=0)\n\n            risk_grad = self.risk_term(\n                game,\n                policies_buff[i],\n                risk_buff[i],\n                opponnet_policies[0],\n                player.tau,\n            )\n\n            policies_buff[i] = pgd.step(policies[i], quantal_grad)\n            risk_buff[i] = pgd.step(risk_policies[i], risk_grad)\n\n        risk_policies = risk_buff\n        policies = policies_buff\n\n    return policies\n</code></pre>"},{"location":"reference/rqe.html#rqequilibrium.rqe.RQE.print_game","title":"print_game  <code>staticmethod</code>","text":"<pre><code>print_game(R1: ndarray, R2: ndarray)\n</code></pre> <p>Print the game matrices for both players.</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>@staticmethod\ndef print_game(R1: np.ndarray, R2: np.ndarray):\n    \"\"\"\n    Print the game matrices for both players.\n    \"\"\"\n    for i in range(R1.shape[0]):\n        row = []\n        for j in range(R1.shape[1]):\n            row.append(f\"{int(R1[i, j])}, {int(R2[i, j])}\")\n        print(\" | \".join(row))\n</code></pre>"},{"location":"reference/rqe.html#rqequilibrium.rqe.RQE.quantal_term","title":"quantal_term","text":"<pre><code>quantal_term(game: ndarray, x: ndarray, p: ndarray, epsilon: float) -&gt; np.array\n</code></pre> <p>Compute the quantal response term for a player given the game matrix, policy and epsilon parameter. Parameters:     game: The game matrix for the player.     x: The current policy of the player.     p: The risk aversion term.     epsilon: The epsilon parameter for bounded rationality. Returns:     The gradient of the quantal term</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>def quantal_term(\n    self, game: np.ndarray, x: np.ndarray, p: np.ndarray, epsilon: float\n) -&gt; np.array:\n    \"\"\"\n    Compute the quantal response term for a player given the game matrix, policy and epsilon parameter.\n    Parameters:\n        game: The game matrix for the player.\n        x: The current policy of the player.\n        p: The risk aversion term.\n        epsilon: The epsilon parameter for bounded rationality.\n    Returns:\n        The gradient of the quantal term\n    \"\"\"\n    return -game @ p + epsilon * self.grad_quantal(x)\n</code></pre>"},{"location":"reference/rqe.html#rqequilibrium.rqe.RQE.risk_term","title":"risk_term","text":"<pre><code>risk_term(game: ndarray, x: ndarray, p: ndarray, y: ndarray, tau: float) -&gt; np.array\n</code></pre> <p>Compute the risk term for a player given the game matrix, policy, and other player's policy.</p> <p>Parameters:</p> Name Type Description Default <code>game</code> <code>ndarray</code> <p>The game matrix for the player.</p> required <code>x</code> <code>ndarray</code> <p>The current policy of the player.</p> required <code>p</code> <code>ndarray</code> <p>The last risk aversion term.</p> required <code>y</code> <code>ndarray</code> <p>The policy of all the other players.</p> required <p>Returns:     The gradient of the risk term</p> Source code in <code>src/rqequilibrium/rqe.py</code> <pre><code>def risk_term(\n    self, game: np.ndarray, x: np.ndarray, p: np.ndarray, y: np.ndarray, tau: float\n) -&gt; np.array:\n    \"\"\"\n    Compute the risk term for a player given the game matrix, policy, and other player's policy.\n\n    Parameters:\n        game: The game matrix for the player.\n        x: The current policy of the player.\n        p: The last risk aversion term.\n        y: The policy of all the other players.\n    Returns:\n        The gradient of the risk term\n    \"\"\"\n    return game.T @ x + (1 / tau) * self.grad_risk(p, y)\n</code></pre>"}]}