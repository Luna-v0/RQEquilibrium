{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ee03bc-5dc9-4b12-be7a-909b2d75a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# Adjusting the location of the src folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  \n",
    "src_path     = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20357ee-8970-4353-ac03-9f40a03d72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ours_pi = []\n",
    "their_pi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3370f965-c11c-4950-aca4-835a160f7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import ArrowStyle, Circle, FancyArrowPatch, Rectangle\n",
    "from matplotlib.path import Path\n",
    "\n",
    "from RQE import RQE, Player\n",
    "\n",
    "colors = sns.color_palette(\"deep\", 10)\n",
    "colors2 = sns.color_palette(\"RdGy\", 10)[6]\n",
    "colors3 = sns.color_palette(\"RdGy\", 10)[9]\n",
    "colors4 = sns.color_palette(\"RdGy\", 10)[4]\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "\n",
    "delta = 1e-15\n",
    "# Testworld=(np.zeros((5,4)),['0___','#___','#__#','#___','1___'])\n",
    "\n",
    "\n",
    "# Small\n",
    "# Testworld=(np.zeros((3,3)),['1__','#__','0__'])\n",
    "# Testworld=(np.zeros((3,3)),['___','#__','$__'])\n",
    "\n",
    "# Med\n",
    "Testworld = (np.zeros((4, 3)), [\"1__\", \"#__\", \"#__\", \"0__\"])\n",
    "# Testworld=(np.zeros((3,3)),['___','#__','$__'])\n",
    "\n",
    "\n",
    "def proj_simplex(v, s=1):\n",
    "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n",
    "    v = np.reshape(v, (v.shape[0]))\n",
    "    (n,) = v.shape  # will raise ValueError if v is not 1-D\n",
    "    # check if we are already on the simplex\n",
    "    if v.sum() == s and np.all(v >= 0):\n",
    "        # best projection: itself!\n",
    "        return v\n",
    "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    # get the number of > 0 components of the optimal solution\n",
    "    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n",
    "    # compute the Lagrange multiplier associated to the simplex constraint\n",
    "    theta = float(cssv[rho] - s) / (rho + 1)\n",
    "    # compute the projection by thresholding v using theta\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w\n",
    "\n",
    "\n",
    "def gradient_player(M, p, x, eps):\n",
    "    return -M.dot(p) - eps * 1 / (x + delta)\n",
    "\n",
    "\n",
    "def gradient_adv(M, p, x, y, tau):\n",
    "\n",
    "    return M.T.dot(x) + 1 / tau * (np.log((p + delta) / (y + delta)) + 1)\n",
    "\n",
    "\n",
    "def solve_game(M1, M2, tau, eps, tau2, eps2, T, gamma=0.1, show=False, tol=1e-4):\n",
    "\n",
    "    (n, m) = M1.shape\n",
    "\n",
    "    x = proj_simplex(np.ones([n, 1]))\n",
    "    px = proj_simplex(np.ones([m, 1]))\n",
    "    y = proj_simplex(np.ones([m, 1]))\n",
    "    py = proj_simplex(np.ones([n, 1]))\n",
    "\n",
    "    payoffs1 = []\n",
    "    payoffs2 = []\n",
    "    strats1 = np.zeros((n, T))\n",
    "    strats2 = np.zeros((m, T))\n",
    "    lastx = 0\n",
    "    lasty = 0\n",
    "    lasti = 100\n",
    "    for t in range(T):\n",
    "        x = proj_simplex(x - gamma * gradient_player(M1, px, x, eps))\n",
    "        px = proj_simplex(px - gamma * gradient_adv(M1, px, x, y, tau))\n",
    "        y = proj_simplex(y - gamma * gradient_player(M2, py, y, eps2))\n",
    "        py = proj_simplex(py - gamma * gradient_adv(M2, py, y, x, tau2))\n",
    "\n",
    "        payoffs1.append(\n",
    "            -x.T.dot(M1).dot(px)\n",
    "            - 1 / tau * (np.sum(y * np.log((y + delta) / (px + delta))))\n",
    "        )\n",
    "        payoffs2.append(\n",
    "            -y.T.dot(M2).dot(py)\n",
    "            - 1 / tau2 * (np.sum(x * np.log((x + delta) / (py + delta))))\n",
    "        )\n",
    "        strats1[:, t] = x[:]\n",
    "        strats2[:, t] = y[:]\n",
    "        diff1 = np.abs(x - lastx)\n",
    "        diff2 = np.abs(y - lasty)\n",
    "        lastx = x\n",
    "        lasty = y\n",
    "        if np.sum(diff1) < tol and np.sum(diff2) < tol:\n",
    "            if t >= 101:\n",
    "                lasti = 100\n",
    "            else:\n",
    "                lasti = t\n",
    "            break\n",
    "\n",
    "    payoff1 = np.mean(payoffs1[-lasti:])\n",
    "    payoff2 = np.mean(payoffs2[-lasti:])\n",
    "    strat1 = np.mean(strats1, axis=1)\n",
    "    strat2 = np.mean(strats2, axis=1)\n",
    "    show = False\n",
    "    if show:\n",
    "        plt.plot(payoffs1)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    eq = RQE(\n",
    "        [\n",
    "            Player(game_matrix=M1, tau=tau, epsilon=eps),\n",
    "            Player(game_matrix=M2.T, tau=tau2, epsilon=eps2),\n",
    "        ],\n",
    "        lr=gamma,\n",
    "        max_iter=T,\n",
    "        br_iters=100,\n",
    "        quantal_function=\"log_barrier\",\n",
    "        risk_function=\"kl_divergence\",\n",
    "    )\n",
    "    pi1_we, pi2_we = eq.optimize()\n",
    "    ours_pi.append(pi1_we)\n",
    "    ours_pi.append(pi2_we)\n",
    "    their_pi.append(x)\n",
    "    their_pi.append(y)\n",
    "    \n",
    "    #print(\"Payoff1: \", np.abs(pi1_we - x) > 0.1)\n",
    "    #print(\"Payoff2: \", np.abs(pi2_we - y) > 0.1)\n",
    "\n",
    "    return payoff1, payoff2, x, y\n",
    "\n",
    "\n",
    "class MAGridWorld:\n",
    "\n",
    "    def __init__(self, world, horizon, taus, eps):\n",
    "\n",
    "        y, x = world[0].shape\n",
    "        self.tau1, self.tau2 = taus\n",
    "        self.eps1, self.eps2 = eps\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_actions = 4\n",
    "        self.desc = world[1]\n",
    "        self.num_agents = 2\n",
    "        self.horizon = horizon\n",
    "        self.gamma = 1.0\n",
    "\n",
    "        if len(self.desc) != self.y:\n",
    "            raise Exception(\"Error: World Description and Grid do not match\")\n",
    "\n",
    "        self.final_bad_states = []\n",
    "        self.final_good_states = [[] for a in range(self.num_agents)]\n",
    "\n",
    "        self.final_bad_nums = []\n",
    "        self.final_good_nums = [[] for a in range(self.num_agents)]\n",
    "\n",
    "        for j in range(self.y):\n",
    "            if len(self.desc[j]) != self.x:\n",
    "                raise Exception(\"Error: World Description and Grid do not match\")\n",
    "            else:\n",
    "                for i in range(self.x):\n",
    "                    if self.desc[j][i] == \"#\":\n",
    "                        self.final_bad_states.append((i, self.y - j - 1))\n",
    "                    for a in range(self.num_agents):\n",
    "                        if self.desc[j][i] == str(a):\n",
    "                            self.final_good_states[a].append((i, self.y - j - 1))\n",
    "                    if self.desc[j][i] == \"$\":\n",
    "                        for a in range(self.num_agents):\n",
    "                            self.final_good_states[a].append((i, self.y - j - 1))\n",
    "\n",
    "        self.state2num = {}\n",
    "        self.num2state = {}\n",
    "        count = 0\n",
    "        for j in range(self.y):\n",
    "            for i in range(self.x):\n",
    "                self.state2num[(i, j)] = count\n",
    "                self.num2state[count] = (i, j)\n",
    "                if (i, j) in self.final_bad_states:\n",
    "                    self.final_bad_nums.append(count)\n",
    "                for a in range(self.num_agents):\n",
    "                    if (i, j) in self.final_good_states[a]:\n",
    "                        self.final_good_nums[a].append(count)\n",
    "                count += 1\n",
    "\n",
    "        self.num_states = count\n",
    "\n",
    "        self.actions = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "\n",
    "        self.Ps = {}\n",
    "        for s1 in range(self.num_states):\n",
    "            for s2 in range(self.num_states):\n",
    "                for a1 in range(self.num_actions):\n",
    "                    for a2 in range(self.num_actions):\n",
    "                        self.Ps[(s1, s2, a1, a2)] = self.getPs(s1, s2, a1, a2)\n",
    "\n",
    "    def getPs(self, s1, s2, a1, a2):\n",
    "\n",
    "        ps = np.zeros((self.num_states, self.num_states))\n",
    "        state1 = self.num2state[s1]\n",
    "        state2 = self.num2state[s2]\n",
    "        true_prob = 0.9\n",
    "        if np.abs(state1[0] - state2[0]) <= 1 and np.abs(state1[1] - state2[1]) <= 1:\n",
    "            true_prob = 0.5\n",
    "\n",
    "        goals = [s1, s2]\n",
    "\n",
    "        possible1 = []\n",
    "        possible2 = []\n",
    "        if s1 not in self.final_bad_nums and s1 not in self.final_good_nums[0]:\n",
    "            for action in self.actions.keys():\n",
    "                d1, d2 = self.actions[action]\n",
    "                next_s1 = (state1[0] + d1, state1[1] + d2)\n",
    "                if next_s1 in self.state2num.keys():\n",
    "                    ns1 = self.state2num[next_s1]\n",
    "                    possible1.append(ns1)\n",
    "                    if a1 == action:\n",
    "                        goals[0] = ns1\n",
    "        else:\n",
    "            possible1 = [s1]\n",
    "\n",
    "        if s2 not in self.final_bad_nums and s2 not in self.final_good_nums[1]:\n",
    "            for act2 in self.actions.keys():\n",
    "                d11, d22 = self.actions[act2]\n",
    "                next_s2 = (state2[0] + d11, state2[1] + d22)\n",
    "                if next_s2 in self.state2num.keys():\n",
    "                    ns2 = self.state2num[next_s2]\n",
    "                    possible2.append(ns2)\n",
    "                    if act2 == a2:\n",
    "                        goals[1] = ns2\n",
    "        else:\n",
    "            possible2 = [s2]\n",
    "\n",
    "        for i in possible1:\n",
    "            for j in possible2:\n",
    "                ps[i, j] = (1 - true_prob) / (len(possible1) * len(possible2))\n",
    "\n",
    "        ps[goals[0], goals[1]] += true_prob\n",
    "        return ps\n",
    "\n",
    "    def getRs(self, s1, s2, a1, a2):\n",
    "\n",
    "        Rs1 = 0.1\n",
    "        Rs2 = 0.1\n",
    "\n",
    "        if s1 in self.final_good_nums[0]:\n",
    "            Rs1 = 1\n",
    "        elif s1 in self.final_bad_nums:\n",
    "            Rs1 = -2\n",
    "        if s2 in self.final_good_nums[1]:\n",
    "            Rs2 = 1\n",
    "        elif s2 in self.final_bad_nums:\n",
    "            Rs2 = -2\n",
    "\n",
    "        return Rs1, Rs2\n",
    "\n",
    "    def KL(self, Qs, Ps, tau):\n",
    "\n",
    "        return -1 / tau * np.log(np.sum(Ps * np.exp(tau * Qs)[:, :, 0]) + delta)\n",
    "\n",
    "    def qlearn(self, T=200, gamma=0.0001, show=False, tol=1e-4):\n",
    "\n",
    "        self.payoffs = [\n",
    "            [\n",
    "                np.zeros((self.num_states, self.num_states, 1))\n",
    "                for t in range(self.horizon)\n",
    "            ]\n",
    "            for a in range(self.num_agents)\n",
    "        ]\n",
    "\n",
    "        self.Qs = [\n",
    "            [\n",
    "                np.zeros(\n",
    "                    [\n",
    "                        self.num_states,\n",
    "                        self.num_states,\n",
    "                        self.num_actions,\n",
    "                        self.num_actions,\n",
    "                    ]\n",
    "                )\n",
    "                for t in range(self.horizon)\n",
    "            ]\n",
    "            for ag in range(self.num_agents)\n",
    "        ]\n",
    "\n",
    "        self.policies = [\n",
    "            [\n",
    "                np.zeros((self.num_states, self.num_states, self.num_actions))\n",
    "                for t in range(self.horizon)\n",
    "            ]\n",
    "            for ag in range(self.num_agents)\n",
    "        ]\n",
    "\n",
    "        print(0)\n",
    "        for s1 in range(self.num_states):\n",
    "            for s2 in range(self.num_states):\n",
    "                for a1 in range(self.num_actions):\n",
    "                    for a2 in range(self.num_actions):\n",
    "                        R1, R2 = self.getRs(s1, s2, a1, a2)\n",
    "                        self.Qs[0][self.horizon - 1][s1, s2, a1, a2] = R1\n",
    "                        self.Qs[1][self.horizon - 1][s1, s2, a1, a2] = R2\n",
    "\n",
    "        for t in range(self.horizon - 1):\n",
    "            print(t + 1)\n",
    "            for s1 in range(self.num_states):\n",
    "                for s2 in range(self.num_states):\n",
    "                    p1, p2, pi1, pi2 = solve_game(\n",
    "                        self.Qs[0][self.horizon - t - 1][s1, s2, :, :],\n",
    "                        self.Qs[1][self.horizon - t - 1][s1, s2, :, :].T,\n",
    "                        self.tau1,\n",
    "                        self.eps1,\n",
    "                        self.tau2,\n",
    "                        self.eps2,\n",
    "                        T,\n",
    "                        gamma / (t + 1),\n",
    "                        show,\n",
    "                        tol,\n",
    "                    )\n",
    "\n",
    "                    self.payoffs[0][self.horizon - t - 1][s1, s2] = p1\n",
    "                    self.payoffs[1][self.horizon - t - 1][s1, s2] = p2\n",
    "                    self.policies[0][self.horizon - t - 1][s1, s2, :] = pi1\n",
    "                    self.policies[1][self.horizon - t - 1][s1, s2, :] = pi2\n",
    "\n",
    "            for s1 in range(self.num_states):\n",
    "                for s2 in range(self.num_states):\n",
    "                    for a1 in range(self.num_actions):\n",
    "                        for a2 in range(self.num_actions):\n",
    "                            R1, R2 = self.getRs(s1, s2, a1, a2)\n",
    "                            self.Qs[0][self.horizon - t - 2][s1, s2, a1, a2] = (\n",
    "                                R1\n",
    "                                + self.gamma\n",
    "                                * self.KL(\n",
    "                                    self.payoffs[0][self.horizon - t - 1],\n",
    "                                    self.Ps[(s1, s2, a1, a2)],\n",
    "                                    self.tau1,\n",
    "                                )\n",
    "                            )\n",
    "                            self.Qs[1][self.horizon - t - 2][s1, s2, a1, a2] = (\n",
    "                                R2\n",
    "                                + self.gamma\n",
    "                                * self.KL(\n",
    "                                    self.payoffs[1][self.horizon - t - 1],\n",
    "                                    self.Ps[(s1, s2, a1, a2)],\n",
    "                                    self.tau2,\n",
    "                                )\n",
    "                            )\n",
    "        print(t + 2)\n",
    "        for s1 in range(self.num_states):\n",
    "            for s2 in range(self.num_states):\n",
    "                p1, p2, pi1, pi2 = solve_game(\n",
    "                    self.Qs[0][0][s1, s2, :, :],\n",
    "                    self.Qs[1][0][s1, s2, :, :].T,\n",
    "                    self.tau1,\n",
    "                    self.eps1,\n",
    "                    self.tau2,\n",
    "                    self.eps2,\n",
    "                    T,\n",
    "                    gamma / self.horizon,\n",
    "                    show,\n",
    "                    tol,\n",
    "                )\n",
    "\n",
    "                self.payoffs[0][0][s1, s2] = p1\n",
    "                self.payoffs[1][0][s1, s2] = p2\n",
    "                self.policies[0][0][s1, s2, :] = pi1\n",
    "                self.policies[1][0][s1, s2, :] = pi2\n",
    "        return\n",
    "\n",
    "    def viewWorld(self, initial=None):\n",
    "\n",
    "        self.fig = plt.figure()\n",
    "        self.gca = self.fig.gca()\n",
    "        plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "        plt.xlim([0, self.x])\n",
    "        plt.ylim([0, self.y])\n",
    "        self.gca.set_xticks([])\n",
    "        self.gca.set_yticks([])\n",
    "        pointer = [0, 0]\n",
    "\n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                center = (i, j)\n",
    "                if (i, j) in self.final_bad_states:\n",
    "                    self.gca.add_patch(\n",
    "                        Rectangle(center, 1, 1, facecolor=colors3, ec=\"k\")\n",
    "                    )\n",
    "                elif (i, j) in self.final_good_states[0] and (\n",
    "                    i,\n",
    "                    j,\n",
    "                ) in self.final_good_states[1]:\n",
    "                    self.gca.add_patch(\n",
    "                        Rectangle(center, 1, 1, facecolor=colors[1], ec=\"k\")\n",
    "                    )\n",
    "                elif (i, j) in self.final_good_states[0] and not (\n",
    "                    i,\n",
    "                    j,\n",
    "                ) in self.final_good_states[1]:\n",
    "                    self.gca.add_patch(Rectangle(center, 1, 1, facecolor=\"r\", ec=\"k\"))\n",
    "                elif (i, j) not in self.final_good_states[0] and (\n",
    "                    i,\n",
    "                    j,\n",
    "                ) in self.final_good_states[1]:\n",
    "                    self.gca.add_patch(Rectangle(center, 1, 1, facecolor=\"b\", ec=\"k\"))\n",
    "                else:\n",
    "                    self.gca.add_patch(\n",
    "                        Rectangle(center, 1, 1, facecolor=colors2, ec=\"k\")\n",
    "                    )\n",
    "\n",
    "                if initial is not None:\n",
    "                    if (i, j) in self.initial[0]:\n",
    "                        self.gca.add_patch(\n",
    "                            Rectangle(center, 1, 1, facecolor=\"r\", ec=\"k\")\n",
    "                        )\n",
    "                    elif (i, j) in self.initial[1]:\n",
    "                        self.gca.add_patch(\n",
    "                            Rectangle(center, 1, 1, facecolor=\"b\", ec=\"k\")\n",
    "                        )\n",
    "\n",
    "                pointer[1] += 1\n",
    "            pointer[0] += 1\n",
    "            pointer[1] = 0\n",
    "\n",
    "    def viewPath(self, initial):\n",
    "\n",
    "        self.viewWorld()\n",
    "        state1 = initial[0]\n",
    "        state2 = initial[1]\n",
    "        t = 0\n",
    "        vertices = []\n",
    "        code = [Path.MOVETO]\n",
    "        update1 = True\n",
    "        update2 = True\n",
    "\n",
    "        for t in range(self.horizon):\n",
    "            start1 = (state1[0] + 0.5, state1[1] + 0.5)\n",
    "            start2 = (state2[0] + 0.5, state2[1] + 0.5)\n",
    "\n",
    "            action1 = np.argmax(\n",
    "                self.policies[0][t][self.state2num[state1], self.state2num[state2], :]\n",
    "            )\n",
    "            action2 = np.argmax(\n",
    "                self.policies[1][t][self.state2num[state1], self.state2num[state2], :]\n",
    "            )\n",
    "\n",
    "            nextstate1 = (\n",
    "                state1[0] + self.actions[action1][0],\n",
    "                state1[1] + self.actions[action1][1],\n",
    "            )\n",
    "            nextstate2 = (\n",
    "                state2[0] + self.actions[action2][0],\n",
    "                state2[1] + self.actions[action2][1],\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                nextstate1 in self.state2num.keys()\n",
    "                and state1 not in self.final_bad_states\n",
    "                and state1 not in self.final_good_states[0]\n",
    "            ):\n",
    "                end1 = (\n",
    "                    start1[0] + self.actions[action1][0],\n",
    "                    start1[1] + self.actions[action1][1],\n",
    "                )\n",
    "                state1 = nextstate1\n",
    "                arrow1 = FancyArrowPatch(\n",
    "                    (start1[0], start1[1]),\n",
    "                    (end1[0], end1[1]),\n",
    "                    lw=2,\n",
    "                    facecolor=\"darkred\",\n",
    "                    ec=\"darkred\",\n",
    "                    arrowstyle=ArrowStyle(\"->,head_length=4,head_width=4\"),\n",
    "                )\n",
    "            else:\n",
    "                end1 = start1\n",
    "                arrow1 = Circle(\n",
    "                    (start1[0], start1[1]),\n",
    "                    0.01,\n",
    "                    lw=1,\n",
    "                    facecolor=\"darkred\",\n",
    "                    ec=\"darkred\",\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                nextstate2 in self.state2num.keys()\n",
    "                and state2 not in self.final_bad_states\n",
    "                and state2 not in self.final_good_states[1]\n",
    "            ):\n",
    "                end2 = (\n",
    "                    start2[0] + self.actions[action2][0],\n",
    "                    start2[1] + self.actions[action2][1],\n",
    "                )\n",
    "                state2 = nextstate2\n",
    "                arrow2 = FancyArrowPatch(\n",
    "                    (start2[0], start2[1]),\n",
    "                    (end2[0], end2[1]),\n",
    "                    lw=2,\n",
    "                    facecolor=\"darkblue\",\n",
    "                    ec=\"darkblue\",\n",
    "                    arrowstyle=ArrowStyle(\"->,head_length=4,head_width=4\"),\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                end2 = start2\n",
    "                arrow2 = Circle(\n",
    "                    (start2[0], start2[1]),\n",
    "                    0.01,\n",
    "                    lw=1,\n",
    "                    facecolor=\"darkblue\",\n",
    "                    ec=\"darkblue\",\n",
    "                )\n",
    "\n",
    "            if update1:\n",
    "                self.gca.add_patch(arrow1)\n",
    "            if update2:\n",
    "                self.gca.add_patch(arrow2)\n",
    "\n",
    "\n",
    "### Plot Configuration\n",
    "\n",
    "# m2=MAGridWorld(Testworld,15,[0.001,0.001],[10,10])\n",
    "# m2.qlearn(1000,0.0005,1,1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916b29ce-b94c-4f66-9592-077eb55eff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd1/Repos/RQEquilibrium/src/RQE.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(p / q) + 1\n",
      "/mnt/hdd1/Repos/RQEquilibrium/src/RQE.py:38: RuntimeWarning: divide by zero encountered in divide\n",
      "  return -1 / x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.009664351851851851),\n",
       " np.float64(0.3060185185185185),\n",
       " np.float64(0.6840277777777778))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MAGridWorld(Testworld, 15, [0.1, 0.1], [10, 10])\n",
    "m1.qlearn(1000, 0.001, 0, 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "920ec5fa-b698-4d6c-bdd1-65ced5dfca61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferença de 0.1 99.0%\n",
      "Diferença de 0.01 87.7%\n"
     ]
    }
   ],
   "source": [
    "res_1 = sum([np.sum(np.abs(x-y) < 0.1) for x,y in zip(ours_pi,their_pi)]) / (len(ours_pi)*4)\n",
    "res_2 = sum([np.sum(np.abs(x-y) < 0.01) for x,y in zip(ours_pi,their_pi)]) / (len(ours_pi)*4)\n",
    "print(f\"Diferença de 0.1 {round(float(res_1)*100,1)}%\")\n",
    "print(f\"Diferença de 0.01 {round(float(res_2)*100,1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04f3b76a-048c-4c6c-b92c-e7bbe4c011f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.48541666666666666)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_maxes_our = np.array([np.argmax(x) for x in ours_pi])\n",
    "arg_maxes_their = np.array([np.argmax(x) for x in their_pi])\n",
    "np.sum(arg_maxes_our == arg_maxes_their) / len(arg_maxes_our)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
